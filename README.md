# Image-Colorization-Using-GAN
VLG Summer Open Project Submission 2022

## Table of Contents: 
* Project Overview
* Prerequisite Theory
* Data Description 
* Libraries used
* Steps followed
* Conclusion
* How to replicate on your device

## Overview of Project:
Colorizing black and white images has always been a difficult task which required a lot of human input and hardcoding. But this gruesome task could be automated with the use of an end-to-end deep learning pipeline. The network can take a black and white image as an input, then produce a fully colored image as output.
Therefore, the final goal of this project is to colorize black and white images with the help of Conditional Generative Adversarial Networks (or CGANs). 

Colorizing black and white images could be broadly categorized as an image to image translation task. Similar to language translation, we can have two different images relay the same information, hence one could be "translated" to the other. This image to image translation task has already been researched in a paper called <a href=https://arxiv.org/pdf/1611.07004.pdf>pix2pix Paper</a>, which again, uses Conditional GAN to fulfil the requirement.

## Prerequisite Theory
Before moving on to the description of model and results, let's first understand some basic things that are used throughout the project.

### Image Spaces
Since we are gonna be working with Black and white images, it is better to move the color space to L\*a\*b from RGB. This is because of two reasons:
* L\*a\*b space requires us to generate only two channels (*a and *b) using the L channel. Compare that with an RGB image, we can see that we are required to generate all three channels. This reduces the complexity of the network, hence reducing the training and testing time drastically.
* Also, it is easier to work with L\*a\*b images, simply because the input and output are very clearly divided where L is the input and \*a\*b is the output, which could be concatencated to produce the final image. No extra conversions are required, as compared to working with RGB image space.

Read more on <a href="https://en.wikipedia.org/wiki/CIELAB_color_space">L\*a\*b here</a>

### Generative Adversarial Networks
Generative Adversarial Networks, or GANs are an approach that's used for unsupervised learning tasks. They are often the backbones of the cool, realistic images generated by A.I. 

<a href="https://www.nvidia.com/en-us/research/ai-demos/">Nvidia AI Demos</a>, <a href="https://this-person-does-not-exist.com/en">ThisPersonDoesNotExist</a>, <a href="https://huggingface.co/spaces/dalle-mini/dalle-mini">DALL.E mini</a>, <a href="https://www.youtube.com/watch?v=6E1_dgYlifc">StyleGAN2 Interpolation Loop</a> to name a few

Instead of going into the in-depth detail of how they work, i'll try to give an intuition around their working.

So, basically, a GAN consists of two 'bots' who compete against each other (hence the adversarial term) where one generates some data, and the other determines whether the generated data is real or fake. Hence, Let's take a simple problem. We ask the generator to generate an image of a tree. Then we take it's output and feed it to the discriminator, which determines whether it's real or fake. Initially, the generator produces random gibberish. But as the process goes on, both the generator and discriminator improves by learning from one another, as both of them are trying to increase the loss of the other, hence learning in the process. The final goal of the generator is to produce outputs that are "indistinguishable from reality". Hence, this forces the discriminator to guess with a probability of 0.5 whether the generated data is real or fake.
As per my understanding, this works because calculating the loss of a generator using traditional loss functions such as L1 or L2 loss are not sufficient to allow the generator to learn to create realistic results. Take the problem at hand, colorizing black and white images. If we train the generator with an L1 loss function instead of a discriminator, It is observed to produce subpar, blurry results (Source: Pix2Pix Paper).
Therefore, to capture the losses for such complex problems, we require a complex and variable method to calculate the loss, which is exactly what the discriminator is training for. It is basically training to judge the generator's output, and nothing else.

For a simple GAN, the loss function is defined as:

$\min_{G}\max_{D}\mathbb{E}_{x}[\log{D(x)}] +  \mathbb{E}_{z}[1 - \log{D(G(z))}]$

Here, we can see that the objective of the Generator (G) is to minimize RHS term, while the discriminator (D) Maximizes the LHS term.

In case of a conditional GAN, alongside some random noise "z", we provide conditional input "x" to allow the GAN to have some supervision. 

Hence, The loss Function becomes:

$\min_{G}\max_{D}\mathbb{E}_{x,y}[\log{D(x,y)}] +  \mathbb{E}_{x,z}[1 - \log{D(x,G(x,z))}]$

Where, x is the label for input data. For B/W to color task, x is the L channel of image, z is random noise and y is the concatencation of \*a\*b channels).

## Data Description:
Since the task is simply colorization of black and white images, we don't need any specific labelled data. This allows us to use any image dataset containing various scenes for training. So i am using the COCO image dataset which contains various different images describing different objects and scenes. i used 8000 images for training and 2000 for testing.
Now, the entire dataset is transformed by 
* Resizing every image to 256x256 pixels
* Applying random horizontal flip (added after training for 350 epochs to see if results improve or not).

The dataset is defined in the following directory structure:
<pre>
.
└── .fastai
    └── data
        └── coco_sample
            └── train_sample
                └── *.jpg (10,000 images in total)
</pre>
## Dependencies:
|     **Name**     |    **pip install Command**   |
|:----------------:|:----------------------------:|
|     **Numpy**    |     **pip install numpy**    |
|  **Matplotlib**  |  **pip install matplotlib**  |
|     **torch**    |     **pip install torch**    |
|  **torchvision** |  **pip install torchvision** |
|     **glob**     |     **pip install glob2**    |
|    **Pillow**    |    **pip install Pillow**    |
| **scikit-image** | **pip install scikit-image** |
|     **tqdm**     |     **pip install tqdm**     |
* Numpy
* Matplotlib
* torch
* torchvision
* PIL
* os module of python
* tqdm
* shutil

## Steps Followed

## Conclusion

## How to replicate on your device
